{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gender_guesser.detector as gender\n",
    "from utils.preprocess import extract_files, extract_zip\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from scipy.io import loadmat\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses python gender_guesser package to benchmark datasets. To run you need to specify the following variables: <br/>\n",
    "1. dataset: can be \"imdb\", \"wiki\", \"scholar\" or \"twitter\"\n",
    "2. path_to_data: path to the zip file \n",
    "3. path_to_output: path where the output folder will be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'scholar' #twitter, wiki, imdb, or scholar\n",
    "path_to_data = 'Scholar.zip' #path to the archive file\n",
    "path_to_output = 'scholar/' #output path to where the data will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_files(dataset, path_to_data, path_to_output) #extract files from archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names(x):\n",
    "    if len(x)>0:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "if dataset == 'twitter':\n",
    "    metadf = pd.DataFrame()\n",
    "    meta_path = path_to_output + path_to_data[:-4] + '/_a_results32langs.zip'\n",
    "    extract_zip(meta_path, meta_path[:-4]+'/')\n",
    "    for file in os.listdir(meta_path[:-4]+'/'):\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(meta_path[:-4]+'/' + file)\n",
    "            metadf = metadf.append(df)\n",
    "    metadf.reset_index(inplace = True)\n",
    "    tw_names = pd.read_csv(path_to_output + path_to_data[:-4] + '/' + 'Twitter_names.csv')\n",
    "    tw_meta = tw_names.merge(metadf[['temp_file', 'indicated_gender']], how='left', left_on='hash', right_on='temp_file')\n",
    "    del metadf, tw_names\n",
    "    tw_meta = tw_meta[['Name', 'indicated_gender']].rename(columns={'indicated_gender':'gender'})\n",
    "    names_df = tw_meta[(tw_meta['gender']=='male') | (tw_meta['gender']=='female')]\n",
    "    \n",
    "elif dataset == 'imdb' or dataset == 'wiki':\n",
    "    if dataset == 'imdb':\n",
    "        path_to_meta =  path_to_output + 'imdb_crop/' + dataset + \".mat\"\n",
    "    else:\n",
    "        path_to_meta =  path_to_output + dataset +'/' + dataset + \".mat\"\n",
    "    mat = loadmat(path_to_meta)  # load mat-file\n",
    "    mdata = mat[dataset]  # variable in mat file\n",
    "    mdtype = mdata.dtype\n",
    "    ndata = {n: mdata[n][0, 0] for n in mdtype.names}\n",
    "    columns = [n for n, v in ndata.items()]# if v.size == ndata['numIntervals']]\n",
    "\n",
    "    dob = mdata['dob'][0,0][0]\n",
    "    photo_taken = mdata['photo_taken'][0,0][0]\n",
    "    full_path = [mdata['full_path'][0,0][0][n][0] for n in range(len(mdata['full_path'][0,0][0]))]\n",
    "    gen = mdata['gender'][0,0][0]\n",
    "    name = np.array(list(map(get_names, mdata['name'][0,0][0])))\n",
    "    face_location = mdata['face_location'][0,0][0]\n",
    "    face_score = mdata['face_score'][0,0][0]\n",
    "    second_face_score = mdata['second_face_score'][0,0][0]\n",
    "    #celeb_id = mdata['celeb_id'][0,0][0]\n",
    "\n",
    "    metadf = pd.DataFrame({\"dob\": dob, \"photo_taken\":photo_taken, \"full_path\":full_path, \"gender\":gen, \"name\":name, \"face_location\":face_location, \"face_score\":face_score, \"second_face_score\":second_face_score})\n",
    "                  #index=celeb_id)\n",
    "    metadf['full_path'] = metadf['full_path'].apply(lambda x: x.split('/')[1])\n",
    "    metadf = metadf[~metadf['gender'].isnull()]\n",
    "    names_df = metadf[['name', 'gender']].replace({\"gender\": {1.:'male', 0:'female'}}).rename(columns={'name':'Name'})\n",
    "    \n",
    "else: #scholar\n",
    "    names_df = pd.DataFrame()\n",
    "    for file in os.listdir(path_to_output):\n",
    "        if file.endswith('.csv'):\n",
    "            scholar_temp = pd.read_csv(path_to_output + file, names=['Name', 'Image1', 'Image2', 'Image3', 'Image4', 'Image5'])\n",
    "            scholar_temp['gender'] = file.split('_')[0]\n",
    "            names_df = names_df.append(scholar_temp)[['Name', 'gender']]\n",
    "            names_df['Name'] = names_df['Name'].str.replace('+', ' ')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = gender.Detector()\n",
    "\n",
    "def get_gender(name):\n",
    "    first_name = name.split(\" \")\n",
    "    return d.get_gender(first_name[0])\n",
    "\n",
    "names_df['predicted_gender'] = names_df['Name'].apply(get_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df['predicted_gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_dict = {'mostly_male':'male', \"mostly_female\":'female', \"andy\":'unknown'}\n",
    "\n",
    "names_df = names_df.replace({\"predicted_gender\": gender_dict})\n",
    "coverage = 1 - names_df['predicted_gender'].value_counts()['unknown']/names_df['predicted_gender'].count()\n",
    "print(f'Coverage: {coverage}')\n",
    "names_df_clean = names_df[names_df['predicted_gender'] != 'unknown']\n",
    "\n",
    "y_true = names_df_clean['gender'].astype(str)\n",
    "y_pred = names_df_clean['predicted_gender'].astype(str)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_true, y_pred))\n",
    "# Print the precision and recall, among other metrics\n",
    "print(metrics.classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "#printing the metrics\n",
    "metrics_dict=metrics.classification_report(y_true, y_pred,output_dict=True)\n",
    "\n",
    "#precision:\n",
    "print('Precision:',round(metrics_dict['weighted avg']['precision'],4))\n",
    "#Recall\n",
    "print('Recall:',round(metrics_dict['weighted avg']['recall'],4))\n",
    "#F1-score\n",
    "print('F1-score:',round(metrics_dict['weighted avg']['f1-score'],4))\n",
    "#accuracy\n",
    "print('Accuracy:',round(metrics_dict['accuracy'],4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
