{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Importing necessary libraries for creating model\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import cv2 # importing open CV library\n",
    "import dlib   # For frontal face identity\n",
    "from PIL import Image\n",
    "import csv\n",
    "import imutils\n",
    "import face_recognition # importing face recognition library\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import sys\n",
    "from zipfile import ZipFile\n",
    "import tarfile\n",
    "from urllib.request import urlopen\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variable to be defined\n",
    "dataset='Gender_Shade'\n",
    "data_path='../../Data/PPB-2017.zip'\n",
    "output_path= '../../Data/Gender_Shade/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data extraction:\n",
    "def extract_data(dataset, data_path, output_path):\n",
    "    if dataset=='imdb':\n",
    "        tar = tarfile.open(data_path,\"r:\")\n",
    "        tar.extractall(output_path)\n",
    "        tar.close()\n",
    "    else:\n",
    "        with ZipFile(data_path, 'r') as zipObj:\n",
    "        # Extract all the contents of zip file \n",
    "            zipObj.extractall(output_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If data is scholar, use this code to extract image_path:\n",
    "extract_data(dataset, data_path, output_path)\n",
    "images=[]\n",
    "def replace_func(text):\n",
    "    chars = ['[',']','u',\"'\"]\n",
    "    for c in chars:\n",
    "        #print(c)\n",
    "        text = text.replace(c, '')\n",
    "    return text\n",
    "#scholar\n",
    "def scholar(file_path,gender):\n",
    "    scholar=pd.read_csv(file_path,header=None)\n",
    "    scholar=scholar.drop(0,axis=1)\n",
    "    Scholar_with_gender= pd.DataFrame()\n",
    "    images_path=[]\n",
    "    for (columnName, columnData) in scholar.iteritems():\n",
    "        images_path.extend(columnData.values)\n",
    "    Scholar_with_gender['Image_name']=images_path\n",
    "    Scholar_with_gender['true_gender']=gender\n",
    "    return Scholar_with_gender\n",
    "\n",
    "scholar_female=scholar(output_path+'female_name_url.csv','f')\n",
    "scholar_male=scholar(output_path+'male_name_url.csv','m')\n",
    "scholar_all = pd.concat([scholar_female,scholar_male],ignore_index=True)\n",
    "scholar_all['Image_name']= scholar_all['Image_name'].apply(lambda x: replace_func(str(x)))\n",
    "images= scholar_all['Image_name']\n",
    "print('Total number of images:', len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the data is one of 5 remaining, use this code to extract image_path\n",
    "extract_data(dataset, data_path, output_path)\n",
    "images=[]\n",
    "if len(next(os.walk(output_path))[1]) == 0: #case when images are in one folder\n",
    "        for image in os.listdir(output_path): \n",
    "            if image[-3:] == 'jpg' or image[-3:] == 'png':\n",
    "                images.append(output_path + image)\n",
    "else: #case when images are in several folders\n",
    "        for path in [output_path + next(os.walk(output_path))[1][n] for n in range(len(next(os.walk(output_path))[1]))]:\n",
    "            if len(next(os.walk(path))[1]) == 0:\n",
    "                for image in os.listdir(path+'/'): \n",
    "                    if image[-3:] == 'jpg' or image[-3:] == 'png':\n",
    "                           images.append(path+'/' + image)\n",
    "            else:\n",
    "                for sub_path in [path+'/' + next(os.walk(path))[1][n] for n in range(len(next(os.walk(path))[1]))]:\n",
    "                    for image in os.listdir(sub_path+'/'): \n",
    "                        if image[-3:] == 'jpg' or image[-3:] == 'png':\n",
    "                               images.append(path+'/' + image)\n",
    "print('Total number of images:', len(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Load and run the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the dataset is scholar used this function to read the images\n",
    "def url_to_image(url):\n",
    "# download the image, convert it to a NumPy array, and then read\n",
    "# it into OpenCV format\n",
    "    resp = urlopen(url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "# return the image\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #############################################################################\n",
    "# loading the trained model and weights\n",
    "json_file = open('../trained model/model.json', 'r')     # Loading the jason file\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights(\"../trained model/model.h5\")    # Loading the weights\n",
    "\n",
    "\n",
    "def detect_faces(image):                    # Create a face detector\n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "    detected_faces = face_detector(image, 1)    # Run detector and get bounding boxes of the faces on image\n",
    "    face_frames = [(x.left(), x.top(),\n",
    "                    x.right(), x.bottom()) for x in detected_faces]  # To identify the frame size\n",
    "    return face_frames\n",
    "\n",
    "\n",
    "def resize_im(image, hight, width):          # Resizing the image\n",
    "    resized_im = image.resize((hight, width))\n",
    "    return resized_im\n",
    "\n",
    "\n",
    "def reshape(numpy_array):            # Reshape the image\n",
    "    d = np.stack(numpy_array)\n",
    "    reshaped = d.reshape(1, d.size)\n",
    "    return reshaped\n",
    "\n",
    "\n",
    "def CNN_baseline(image_path):\n",
    "    output = pd.DataFrame()\n",
    "    img= cv2.imread(image_path)\n",
    "    #img= url_to_image(image_path)     #uncomment this if your dataset is scholar\n",
    "    detected_faces = detect_faces(img)\n",
    "    for n, face_rect in enumerate(detected_faces):\n",
    "                    fa = Image.fromarray(img).crop(face_rect)   # Cropping the face or faces\n",
    "                    s = np.array(resize_im(fa, 100,100))\n",
    "                    c = cv2.cvtColor(s, cv2.COLOR_BGR2GRAY)    # Converting the image\n",
    "                    re_data = reshape(c)\n",
    "                    # Save the converted image data\n",
    "                    dic = { i : re_data[0][i] for i in range(0, len(re_data[0]) ) }\n",
    "                    output = output.append(dic, ignore_index=True)\n",
    "                    cv2.waitKey(0)\n",
    "                    cv2.destroyAllWindows()    # Need to kill the image window\n",
    "    df_x= output.iloc[:,0:].values.reshape(len(output),100,100,1)\n",
    "    f_x = np.array(df_x)  # Need to convert as a array\n",
    "    loaded_model.compile(loss='categorical_crossentropy',optimizer = 'adadelta', metrics = ['accuracy'])\n",
    "    prob = np.asmatrix(loaded_model.predict(f_x))\n",
    "    if prob[0,0] > prob[0,1]:\n",
    "             gender= 'f'\n",
    "    if prob[0,0] < prob[0,1]:\n",
    "             gender='m'\n",
    "    \n",
    "    return gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path+dataset+'.csv', 'w', newline='') as myfile:             # Here output written in the file\n",
    "        wr = csv.writer(myfile,quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow(['Image_name','Pred_Gender'])\n",
    "        for imagelocation in images:                        # Get the images location\n",
    "                try:\n",
    "                    CNN_baseline_gender=CNN_baseline(imagelocation)   # return the predicted gender\n",
    "                    wr = csv.writer(myfile,quoting=csv.QUOTE_ALL)\n",
    "                    wr.writerow([imagelocation,CNN_baseline_gender])\n",
    "                except:\n",
    "                    wr = csv.writer(myfile,quoting=csv.QUOTE_ALL)\n",
    "                    wr.writerow([imagelocation,'u'])   #if there is an error in returning gender , write undefined\n",
    "                    print(\"Oops!\",sys.exc_info()[0],\"occured.\")\n",
    "                    #print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Extract the annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for OUI Dataset\n",
    "def oui_annotation_extraction(annotation_path,annotation_output_path):\n",
    "    extract_data(dataset, annotation_path, annotation_output_path)  #unzip the file \n",
    "    files=[annotation_output_path+'/'+name for name in os.listdir(annotation_output_path) if os.path.isfile(os.path.join(annotation_output_path, name))]\n",
    "    df = pd.DataFrame()\n",
    "    for file in files:\n",
    "        data = pd.read_csv(file, sep=\"\\t\")  #load each file as dataframe\n",
    "        df= df.append(data, ignore_index = True)\n",
    "    df= df.astype(str)\n",
    "    df['Image_name']=df['user_id']+'/'+ df['face_id']+'.'+df['original_image'] # make Image name similar to the one in the image path\n",
    "    df= df[['Image_name','gender']] # keep only gender and imagename\n",
    "    df=df.rename(columns={\"gender\": \"true_gender\"})\n",
    "    df['true_gender']=df['true_gender'].apply(lambda x: 'u' if x=='nan' else x)\n",
    "    return df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oui_annotation= oui_annotation_extraction('../../Data/OUI_annotations.zip','../../Data/OUI_annotations')\n",
    "print(oui_annotation.head(2),'\\n')\n",
    "print('the shape of the annotation dataframe is',oui_annotation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Gender_shade dataset\n",
    "shade_annotation= pd.read_csv('../../Data/PPB-2017-metadata.csv')\n",
    "shade_annotation= shade_annotation[['filename','gender']]\n",
    "shade_annotation.columns = ['Image_name', 'true_gender']\n",
    "shade_annotation['true_gender']= shade_annotation['true_gender'].apply(lambda x: 'f' if x=='Female' else 'm')\n",
    "print(shade_annotation.head(2),'\\n')\n",
    "print('the shape of the annotation dataframe is',shade_annotation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For IMDB and wiki datasets\n",
    "############################\n",
    "### for .mat file for IMDB ###\n",
    "def get_names(x):\n",
    "    if len(x)>0:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return ''\n",
    "dataset ='imdb'\n",
    "### for .mat file for imdb and wiki ###\n",
    "if dataset == 'imdb' or dataset == 'wiki':\n",
    "    path_to_meta = path_to_output + dataset + \".mat\"\n",
    "    mat = loadmat(path_to_meta)  # load mat-file\n",
    "    mdata = mat[dataset]  # variable in mat file\n",
    "    mdtype = mdata.dtype\n",
    "    ndata = {n: mdata[n][0, 0] for n in mdtype.names}\n",
    "    columns = [n for n, v in ndata.items()]# if v.size == ndata['numIntervals']] \n",
    "    full_path = [mdata['full_path'][0,0][0][n][0] for n in range(len(mdata['full_path'][0,0][0]))]\n",
    "    gender = mdata['gender'][0,0][0]\n",
    "    if dataset == 'imdb':\n",
    "        imdb_annotation = pd.DataFrame({ \"Image_name\":full_path, \"true_gender\":gender})\n",
    "    elif dataset=='wiki':\n",
    "        wiki_annotation = pd.DataFrame({ \"Image_name\":full_path, \"true_gender\":gender})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###For twitter dataset\n",
    "\n",
    "metadf = pd.DataFrame()\n",
    "meta_path = '/Users/nadabeili/Desktop/twitter/_a_results32langs/'\n",
    "for file in os.listdir(meta_path):\n",
    "    df = pd.read_csv(meta_path + file)\n",
    "    metadf = metadf.append(df)\n",
    "    \n",
    "metadf.reset_index(inplace = True)\n",
    "metadf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Precision, recall, F1_score\n",
    "#read the result_file\n",
    "df_pred=pd.read_csv(output_path+dataset+'.csv')\n",
    "## make image names similar to the annotation\n",
    "df_pred['Image_name']= df_pred['Image_name'].apply(lambda x: x.split('/')[-1])\n",
    "##\n",
    "result_data= pd.merge(shade_annotation,df_pred, how= 'inner', on='Image_name')\n",
    "result_data.to_csv('../../Data/all_result_Gender_shade.csv')\n",
    "result_data= result_data[result_data.Pred_Gender !='u']\n",
    "# True values\n",
    "y_true = result_data['true_gender']\n",
    "# Predicted values\n",
    "y_pred = result_data['Pred_Gender']\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "# Print the precision and recall, among other metrics\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "#printing the metrics\n",
    "metrics_dict=classification_report(y_true, y_pred,output_dict=True)\n",
    "\n",
    "#precision:\n",
    "print('Precision:',round(metrics_dict['weighted avg']['precision'],4))\n",
    "#Recall\n",
    "print('Recall:',round(metrics_dict['weighted avg']['recall'],4))\n",
    "#F1-score\n",
    "print('F1-score:',round(metrics_dict['weighted avg']['f1-score'],4))\n",
    "#accuracy\n",
    "print('Accuracy:',round(metrics_dict['accuracy'],4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36workshop",
   "language": "python",
   "name": "p36workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
