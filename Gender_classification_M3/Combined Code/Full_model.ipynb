{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from m3inference import M3Inference\n",
    "from collections import OrderedDict\n",
    "#import pprint\n",
    "#import urllib.request\n",
    "#import random\n",
    "import re\n",
    "import os\n",
    "import csv \n",
    "import sys\n",
    "sys.path.append('../utils/')\n",
    "import string \n",
    "import operator\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from io import BytesIO\n",
    "from scipy.io import loadmat\n",
    "from m3preprocess import extract_files, preprocess_images, extract_zip, get_scholar_data\n",
    "from PIL import Image\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows you to run M3 on image datasets and get the performance statistics. <a href=\"https://github.com/euagendas/m3inference\">This M3 implementation code </a> is used.\n",
    "\n",
    "You should specify the following variables when running the notebook:\n",
    "- **dataset**: which dataset you use. Can be **only** one of the following: wiki, IMDB, Twitter, Scholar, OUI, Gender Shade. For new datasets please modify the code for extracting the images and reading the annotations file\n",
    "- **path_to_data**: path to the data (original data is saved here: 175.238.89:/bigdisk/gender_inference/Unpruned_data/) in .zip or .tar format\n",
    "- **path_to_output**: path to where the data will be extracted, as well as where data.jsonl and result.csv will be saved\n",
    "- **with_name**: boolean parameter to specify is M3 should run with names (by default is False)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Necessary variables to change \n",
    "dataset = 'twitter' #twitter, wiki, imdb, gender_shade, scholar or oui\n",
    "path_to_data = 'Twitter_full.zip'\n",
    "path_to_output = 'twitter/'\n",
    "with_name = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Unpack and extract images paths\n",
    "path_to_images = extract_files(dataset, path_to_data, path_to_output)\n",
    "#path_to_images = path_to_output + 'wiki/'\n",
    "\n",
    "images = []\n",
    "if len(next(os.walk(path_to_images))[1]) == 0: #case when images are in one folder\n",
    "    for image in os.listdir(path_to_images): \n",
    "        if image[-3:] == 'jpg' or image[-3:] == 'png':\n",
    "            images.append(path_to_images + image)\n",
    "elif dataset == 'scholar':\n",
    "    scholar = get_scholar_data(path_to_images, path_to_output)\n",
    "    for file in os.listdir('scholar/'):\n",
    "        if file.endswith('.png'):\n",
    "            images.append('scholar/'+file)\n",
    "    \n",
    "else: #case when images are in several folders\n",
    "    for path in [path_to_images + next(os.walk(path_to_images))[1][n] for n in range(len(next(os.walk(path_to_images))[1]))]:\n",
    "        for image in os.listdir(path+'/'): \n",
    "            if image[-3:] == 'jpg' or image[-3:] == 'png':\n",
    "                images.append(path+'/' + image)\n",
    "            \n",
    "print('Total number of images:', len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# extract annotations file\n",
    "\n",
    "### for .mat file for IMDB ###\n",
    "def get_names(x):\n",
    "    if len(x)>0:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "### for .mat file for imdb and wiki ###\n",
    "if dataset == 'imdb' or dataset == 'wiki':\n",
    "    if dataset == 'imdb':\n",
    "        path_to_meta =  path_to_output + 'imdb_crop/' + dataset + \".mat\"\n",
    "    else:\n",
    "        path_to_meta =  path_to_output + dataset +'/' + dataset + \".mat\"\n",
    "    mat = loadmat(path_to_meta)  # load mat-file\n",
    "    mdata = mat[dataset]  # variable in mat file\n",
    "    mdtype = mdata.dtype\n",
    "    ndata = {n: mdata[n][0, 0] for n in mdtype.names}\n",
    "    columns = [n for n, v in ndata.items()]# if v.size == ndata['numIntervals']]\n",
    "\n",
    "    dob = mdata['dob'][0,0][0]\n",
    "    photo_taken = mdata['photo_taken'][0,0][0]\n",
    "    full_path = [mdata['full_path'][0,0][0][n][0] for n in range(len(mdata['full_path'][0,0][0]))]\n",
    "    gender = mdata['gender'][0,0][0]\n",
    "    name = np.array(list(map(get_names, mdata['name'][0,0][0])))\n",
    "    face_location = mdata['face_location'][0,0][0]\n",
    "    face_score = mdata['face_score'][0,0][0]\n",
    "    second_face_score = mdata['second_face_score'][0,0][0]\n",
    "    #celeb_id = mdata['celeb_id'][0,0][0]\n",
    "\n",
    "    metadf = pd.DataFrame({\"dob\": dob, \"photo_taken\":photo_taken, \"full_path\":full_path, \"gender\":gender, \"name\":name, \"face_location\":face_location, \"face_score\":face_score, \"second_face_score\":second_face_score})\n",
    "                  #index=celeb_id)\n",
    "    metadf['full_path'] = metadf['full_path'].apply(lambda x: x.split('/')[1])\n",
    "    \n",
    "\n",
    "elif dataset == 'oui':\n",
    "    extract_zip('OUI_annotations.zip', 'OUI')\n",
    "    metadf = pd.DataFrame()\n",
    "    for file in ['OUI/fold_frontal_0_data.txt', 'OUI/fold_frontal_1_data.txt', 'OUI/fold_frontal_2_data.txt', 'OUI/fold_frontal_3_data.txt', 'OUI/fold_frontal_4_data.txt']:\n",
    "        data = pd.read_csv(file, sep=\"\t\", header=None)\n",
    "        data.columns = data.iloc[0]\n",
    "        data.drop(0, inplace=True)\n",
    "        metadf = pd.concat([metadf, data])\n",
    "        \n",
    "elif dataset == 'twitter':\n",
    "    metadf = pd.DataFrame()\n",
    "    meta_path = path_to_output + path_to_data[:-4] + '/_a_results32langs.zip'\n",
    "    extract_zip(meta_path, meta_path[:-4]+'/')\n",
    "    for file in os.listdir(meta_path[:-4]+'/'):\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(meta_path[:-4]+'/' + file)\n",
    "            metadf = metadf.append(df)\n",
    "    metadf.reset_index(inplace = True)\n",
    "    tw_names = pd.read_csv(path_to_output + path_to_data[:-4] + '/' + 'Twitter_names.csv')\n",
    "    \n",
    "else:\n",
    "    metadf = scholar.copy()\n",
    "    \n",
    "             \n",
    "display(metadf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in images:\n",
    "    tw_hash = image.split('/')[-1][:-4]\n",
    "    print(tw_names[tw_names['hash']==tw_hash]['Name'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# preprocess images and create a json file for M3\n",
    "\n",
    "data = {}\n",
    "data['images'] = []\n",
    "for image in images:\n",
    "    valid = preprocess_images(image, 224, 224, skip = False) # specify skip = False if size condition should be ignored (height+width>=400)\n",
    "    if valid == False:\n",
    "        pass\n",
    "    else:\n",
    "        if dataset == 'scholar':\n",
    "            name = image.split('/')[1][:-11].replace('+', ' ')\n",
    "        elif dataset == 'wiki' or dataset == 'imdb':\n",
    "            name = metadf[metadf['full_path'] == image.split('/')[-1]]['name'].values[0]\n",
    "        elif dataset == 'twitter':\n",
    "            tw_hash = image.split('/')[-1][:-4]\n",
    "            name = tw_names[tw_names['hash']==tw_hash]['Name'].values\n",
    "            if len(name)>0:\n",
    "                name = name[0]\n",
    "            else:\n",
    "                name = ''\n",
    "            if type(name)!=str:\n",
    "                print(name)\n",
    "                \n",
    "        if with_name == True:\n",
    "            data['images'].append({\n",
    "                \"description\":\"\", \n",
    "                \"id\": image.split('/')[-1],\n",
    "                \"img_path\": image, \n",
    "                \"lang\": \"en\", \n",
    "                \"name\": name, \n",
    "                \"screen_name\": \"\"\n",
    "            })\n",
    "        else:\n",
    "            data['images'].append({\n",
    "                \"description\":\"\", \n",
    "                \"id\": image.split('/')[-1],\n",
    "                \"img_path\": image, \n",
    "                \"lang\": \"en\", \n",
    "                \"name\": \"\", \n",
    "                \"screen_name\": \"\"\n",
    "            })\n",
    "    \n",
    "with open(path_to_output + path_to_data[:-4] + '/data.jsonl', 'w') as json_file: # json file for m3 is created  \n",
    "    json.dump(data, json_file)\n",
    "\n",
    "_json = path_to_output+ path_to_data[:-4] + '/data.jsonl'\n",
    "print('Json saved at ', _json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# run M3 and infer gender\n",
    "\n",
    "def M3_inference(path_to_output, _json):\n",
    "    try:\n",
    "        with open(_json) as json_file:\n",
    "            data = json.load(json_file)\n",
    "        pred = m3.infer(data['images']) #get the predictions from json file\n",
    "        #disc=pprint.pprint(pred)\n",
    "        with open(path_to_output+ path_to_data[:-4] +'predictions.json', 'w') as pred_file:\n",
    "            json.dump(pred, pred_file, indent=3)\n",
    "        print(f'Predictions are finished for {len(pred)} images')\n",
    "\n",
    "        with open(path_to_output+ path_to_data[:-4]+'result.csv', 'w', newline='') as output:  # output file is created\n",
    "            wr = csv.writer(output,quoting=csv.QUOTE_ALL)\n",
    "            wr.writerow(['Imagename','Predicted_Gender', 'Is_Org']) #header row\n",
    "            for tup in pred.items():\n",
    "                gender_conf = tup[1]['gender'] #extracting predictions for gender\n",
    "                gender = max(gender_conf.items(), key=operator.itemgetter(1)) #50% threshold, choosing gender with max confidence score\n",
    "                org = tup[1]['org'] #extracting predictions for org (if several people are presented on image)\n",
    "                is_org = False #boolean variable will be stored in the output file\n",
    "                if org['is-org'] > 0.5:\n",
    "                    is_org = True\n",
    "                wr.writerow([tup[0], gender[0], is_org]) #writing a row for every image with image name, predicted gender and is_org flag\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print (e)\n",
    "\n",
    "m3 = M3Inference()\n",
    "M3_inference(path_to_output, _json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# extract the result of predictions\n",
    "\n",
    "def org(row):\n",
    "    if row['Is_Org'] == True:\n",
    "        return 'orga'\n",
    "    else:\n",
    "        return row['Predicted_Gender']\n",
    "\n",
    "results = pd.read_csv(path_to_output+ path_to_data[:-4]+'result.csv')\n",
    "if dataset == 'oui':\n",
    "    results['Imagename'] = results['Imagename'].apply(lambda x: '.'.join(x.split('.')[-2:]))\n",
    "    results_merged = pd.merge(results, metadf, left_on='Imagename', right_on='original_image')\n",
    "    results_merged['Predicted_Gender'] = results_merged['Predicted_Gender'].apply(lambda x: x[0])\n",
    "    \n",
    "elif dataset == 'twitter':\n",
    "    results['Imagename'] = results['Imagename'].apply(lambda x: x[:-4]) #for Twitter need to remove .png\n",
    "    results_merged = pd.merge(results, metadf, left_on='Imagename', right_on='temp_file')\n",
    "    results_merged['Predicted_Gender'] = results_merged.apply(lambda x: org(x), axis=1)\n",
    "    results_merged = results_merged[results_merged['indicated_gender:confidence'] >= 0.8][(results_merged['indicated_gender']=='male') | (results_merged['indicated_gender']=='female') | (results_merged['indicated_gender']=='orga')]\n",
    "    results_merged = results_merged.rename({\"indicated_gender\":\"gender\"}, axis=1)\n",
    "\n",
    "elif dataset == 'imdb' or dataset =='wiki':\n",
    "    results_merged = pd.merge(results, metadf, left_on='Imagename', right_on='full_path') #merge results and annotations\n",
    "    results_merged['gender'] = results_merged['gender'].apply(lambda x: \"female\" if x == 0 else \"male\") #for wiki\n",
    "    \n",
    "elif dataset == 'scholar':\n",
    "    results_merged = pd.merge(results, scholar, left_on='Imagename', right_on='file_name').drop_duplicates()\n",
    "    \n",
    "results_merged\n",
    "    \n",
    "#results_merged = results_merged[(results_merged['gender']=='f') | (results_merged['gender']=='m') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell is used for scholar data, to see how predicted labels vary for 1 person ###\n",
    "def f(group):\n",
    "    return group['Predicted_Gender'].value_counts().min() / group['Predicted_Gender'].value_counts().sum()\n",
    "\n",
    "if dataset == 'scholar':\n",
    "    group_df = results_merged[['Name', 'Predicted_Gender', 'gender']].groupby(['Name'])\n",
    "    variation = group_df.apply(lambda x: f(x))\n",
    "    variation.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# print the confusion matrix and performance metrics\n",
    "# True values\n",
    "y_true = results_merged['gender']\n",
    "# Predicted values\n",
    "y_pred = results_merged['Predicted_Gender']\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_true, y_pred))\n",
    "# Print the precision and recall, among other metrics\n",
    "print(metrics.classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "#printing the metrics\n",
    "metrics_dict=metrics.classification_report(y_true, y_pred,output_dict=True)\n",
    "\n",
    "#precision:\n",
    "print('Precision:',round(metrics_dict['weighted avg']['precision'],4))\n",
    "#Recall\n",
    "print('Recall:',round(metrics_dict['weighted avg']['recall'],4))\n",
    "#F1-score\n",
    "print('F1-score:',round(metrics_dict['weighted avg']['f1-score'],4))\n",
    "#accuracy\n",
    "print('Accuracy:',round(metrics_dict['accuracy'],4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
